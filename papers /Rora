https://papers.ssrn.com/sol3/papers.cfm?abstract_id=6101568


Rotational Rank Adaptation (RoRA):
Spectral–Orthogonal Geometry for Robust Model
Merging
Agus Sudjianto1,2, Nurhadi 3 and Kannan Venkataramanan4
1H2O.ai,
2Center for Trustworthy AI Through Model Risk Management,
University of North Carolina Charlotte
3Vidio.com
4Solytics Partners
January 20, 2026
Abstract
Merging fine-tuned neural networks offers an appealing path to combining diverse capabil
ities without retraining, yet naive weight averaging often fails catastrophically when models
drift into geometrically incompatible regions of parameter space. We address this challenge
through a spectral–orthogonal lens: decomposing weight matrices as W = UΣV ⊤ reveals
that adaptation involves both spectral changes (modifying singular values Σ) and orthogonal
changes (rotating bases U,V ). We argue that when fine-tuning strong base models, many
practical tasks require only basis reorientation; allowing unnecessary spectral drift leads
to overfitting and merge failures. We propose Rotational Rank Adaptation (RoRA),
a parameter-efficient method that constrains updates to orthogonal transformations via
low-rank skew-symmetric generators in the Lie algebra so(d). This preserves feature energy
and enables stable merging through manifold-aware interpolation. We demonstrate RoRA’s
effectiveness across four distinct regimes: multi-task learning on MNIST, adaptation to novel
classes, small-data fine-tuning on CIFAR-10, and missing-modality transfer from grayscale to
RGB images. Our results show that geometric, spectrum-preserving adaptation consistently
outperforms additive low-rank methods when spectral change is unnecessary, while identifying
specific scenarios where spectral adaptation remains beneficial.
Keywords: Parameter-efficient fine-tuning; model merging; orthogonal adaptation; isospectral
learning; Lie algebra; low-rank adaptation; geometric deep learning
1 Introduction
The ability to merge independently fine-tuned neural networks without retraining is crucial for
scalable AI systems. Recent work on model soups [Wortsman et al., 2022] and task arithmetic
[Ilharco et al., 2023] demonstrates that simple weight-space averaging can succeed when solutions
occupy a connected mode. However, merging frequently fails when models become geometrically
misaligned: linearly averaging nearly orthogonal parameter vectors contracts weight norms and
collapses singular values, destroying the learned representations—a failure mode we term the
Mediocrity Trap.
Consider two models fine-tuned from a shared base for different tasks. Their weight matrices
W1 and W2 may diverge both in their singular value spectra (capacity) and in their left/right
1
singular subspaces (orientation). When we average 1
2(W1 + W2), misaligned singular vectors
interfere destructively, reducing effective rank and collapsing performance. This geometric insight
motivates a fundamental question: What types of adaptation are necessary for a given task, and
how should we merge accordingly?
1.1 The Spectral–Orthogonal Decomposition
We propose viewing adaptation through a spectral–orthogonal decomposition. For a weight
matrix W ∈ Rm×d with singular value decomposition (SVD) W = UΣV ⊤, we distinguish:
• Spectral adaptation: Changes to the singular values Σ, reflecting capacity expansion or
contraction in different directions.
• Orthogonal adaptation: Changes to the basis matrices (U,V), reflecting feature space
reorientation.
Our central hypothesis is:
When fine-tuning a strong base model, many tasks can be solved by reorienting an
already-capable feature basis. Allowing unnecessary spectral drift typically overfits
noise, particularly under data scarcity, and leads to merge failures. Orthogonal-only
adaptation should be preferred in such regimes.
1.2 Contributions
We introduce Rotational Rank Adaptation (RoRA), a parameter-efficient fine-tuning
method that constrains adaptation to orthogonal transformations. RoRA parameterizes rotations
via low-rank skew-symmetric generators, enabling efficient O(dr2 + r3) computation and stable
geometric merging. Our key contributions are:
1. A spectral–orthogonal framework for understanding when different adaptation strategies
succeed or fail.
2. RoRA: an efficient orthogonal adaptation method with provable energy preservation and
mergability guarantees.
3. A geometric merging algorithm for RoRA modules via subspace alignment and Lie algebra
interpolation.
4. Comprehensive empirical validation across four distinct regimes:
• MNIST multi-task: RoRA achieves 99.2% accuracy on simultaneous even/odd and
bit-parity classification.
• MNIST Add-9: When base model lacks novel features, LoRA shows marginal gains
(+0.09%).
• CIFAR-10 small data: RoRA outperforms LoRA by 10% under extreme data
scarcity (250 samples/class).
• CIFAR-10 grayscale→RGB: RoRA dramatically outperforms LoRA (+16%) in
missing modality transfer, showing geometric adaptation recovers color information
effectively.
2
Our findings establish clear decision criteria: use RoRA when the base model has sufficient
representational capacity and requires only reorientation; use LoRA when novel spectral directions
are genuinely needed.
2 Related Work
Model Merging, Model Soups, and Mode Connectivity. A growing body of work
studies how to combine independently fine-tuned models without retraining. Model soups
[Wortsman et al., 2022] show that averaging weights across fine-tuned checkpoints can improve
accuracy when solutions lie in a connected low-loss region. Task arithmetic [Ilharco et al.,
2023] formalizes merging through task vectors added in weight space, implicitly assuming
approximate linearity in the local basin. When these assumptions break, merging can fail due to
misalignment and symmetries. Re-basin methods explicitly address permutation symmetries
before interpolation [Ainsworth et al., 2023], while Fisher-weighted merging [Matena and Raffel,
2022] reweights layers/parameters to mitigate destructive interference. Our work complements
these approaches by identifying a specific geometric failure mode—destructive interference from
orthogonal misalignment leading to effective rank collapse—and by proposing merge-friendly
updates that operate explicitly on the orthogonal degrees of freedom.
Parameter-Efficient Fine-Tuning (PEFT). PEFT methods adapt large models by training
a small number of parameters while freezing the backbone. LoRA [Hu et al., 2022] is the
dominant paradigm, using additive low-rank updates that can change both the singular values
and singular vectors of the effective weight map. Subsequent variants improve efficiency and
stability, including QLoRA [Dettmers et al., 2023] for quantized training and AdaLoRA [Zhang
et al., 2023] for adaptive rank allocation. Alternative PEFT families include adapters [Houlsby
et al., 2019] and prefix tuning [Li and Liang, 2021]. While effective, these approaches generally do
not isolate spectral versus orthogonal components of adaptation, and thus can induce unnecessary
spectral drift in regimes where reorientation alone suffices.
Isospectral and Orthogonal PEFT. Recent work on isospectral transport proposes con
straining adaptation to orthogonal transformations that preserve singular values. In particular,
Sudjianto [2026] introduce GIFT (Geometric Isospectral Fine-Tuning), which performs orthogo
nal right-multiplication using low-rank skew-symmetric generators parameterized via the Cayley
transform and implemented efficiently through Woodbury-accelerated activation rotations. This
design yields near-LoRA training efficiency while demonstrating applicability at transformer
scale. RoRA and GIFT share the central geometric principle that many adaptations can be
realized through controlled orthogonal motion; however, they differ in emphasis. GIFT primarily
targets computational efficiency and large-scale deployment, whereas RoRA develops a spectral
orthogonal framework for diagnosing when orthogonal adaptation is sufficient, and contributes a
merging-oriented formulation in the Lie algebra that supports manifold-aware interpolation of
orthogonal adapters.
Orthogonal Methods and Matrix-Manifold Optimization. Orthogonality has long been
used to stabilize optimization and preserve conditioning in deep networks. Classic results
connect orthogonal structure to improved learning dynamics [Saxe et al., 2014], and optimization
on matrix manifolds provides principled tools for constrained updates [Absil et al., 2009].
Orthogonality regularization has also been explored in recurrent and convolutional settings
for stability and robustness [Vorontsov et al., 2017, Li et al., 2019]. Our contribution differs
from methods that impose orthogonality on the base weights: we keep the backbone frozen and
3
instead learn a low-rank orthogonal adaptation transform, yielding a PEFT mechanism that
preserves feature energy and is explicitly designed to be mergeable.
Spectral Diagnostics and Capacity Control. Spectral control has been used to improve
generalization and stability, including spectral norm regularization [Yoshida and Miyato, 2017]
and spectral normalization [Miyato et al., 2018]. Random-matrix-theoretic analyses further
link singular value structure to effective capacity and implicit self-regularization [Martin and
Mahoney, 2021]. RoRA operationalizes these perspectives by separating spectral and orthogonal
degrees of freedom and by connecting merge failures to spectral contraction induced by orthogonal
misalignment (the Mediocrity Trap), motivating adaptation rules that preserve spectra when
spectral change is unnecessary.
3 Spectral–Orthogonal Framework
3.1 Decomposing Weight Adaptation
Consider a base model with weight matrix Wb ∈ Rm× d and its SVD Wb = UbΣbV⊤
b . After
f
ine-tuning for a task, we obtain Wt = UtΣtV ⊤
t . The change from base to task can be decomposed
as:
Wt =UtΣtV⊤
t = (UtU⊤
b )
left rotation
UbΣtV ⊤
t
spectrum in base coords
This reveals three adaptation modes:
(VbV ⊤
t )
right rotation
1. Pure spectral: Σt ̸ = Σb but Ut ≈ Ub,Vt ≈ Vb (scaling existing directions)
(1)
2. Pure orthogonal: Σt ≈ Σb but UtU⊤
b ,VbV⊤t are non-trivial rotations (reorienting basis)
3. Mixed: Both spectrum and basis change significantly
Definition 1 (Orthogonal-Sufficient Task). A task is orthogonal-sufficient relative to base Wb if
there exist orthogonal matrices RL ∈ O(m) and RR ∈ O(d) such that the optimal adapted weight
satisfies
W∗
t ≈ RLWbR⊤
R,
preserving singular values up to numerical tolerance.
(2)
Definition 2 (Spectral-Necessary Task). A task is spectral-necessary if achieving target per
formance requires ∥Σt − Σb∥F/∥Σb∥F > ϵ for non-negligible ϵ, indicating the base capacity
distribution is inadequate.
3.2 The Mediocrity Trap
Whenmerging two adapted models via naive averaging ¯
W =1
2(W1+W2), geometric misalignment
causes destructive interference:
Proposition 1 (Spectral Contraction in Naive Averaging). Let W1 = U1ΣV ⊤
1 and W2 =U2ΣV⊤
2
share singular values Σ but have orthogonal subspaces: U⊤
1U2 = 0. Then the largest singular
value of ¯
W =1
2(W1+W2) satisfies
σmax( ¯
W)≤ 1
2σmax(Σ),
(3)
4
resulting in at least 50% capacity loss despite individual models having full capacity.
Proof. By orthogonality of U1,U2 and V1,V2, the squared Frobenius norm satisfies
∥ ¯W∥2
F = 1
4∥W1∥2
F + 1
4∥W2∥2
F = 1
2∥Σ∥2
F.
Since σmax ≤ ∥W∥F, we have σmax( ¯W) ≤ 1
√
2
(4)
σmax(Σ). Tighter analysis via singular subspace
angles gives the stated bound.
This motivates adaptation strategies that preserve geometric structure during merging.
4 Rotational Rank Adaptation (RoRA)
4.1 Method Overview
RoRA implements orthogonal adaptation by right-multiplying frozen base weights W ∈ Rm×d
with a rotation matrix R ∈ SO(d):
y =W(R⊤x).
(5)
Since R is orthogonal, ∥R⊤x∥ = ∥x∥ and feature energy is preserved. Critically, the singular
values of WR⊤ equal those of W, ensuring spectral invariance.
4.2 Low-Rank Parameterization
Full-rank rotations require O(d2) parameters. We parameterize R via its Lie algebra so(d) using
a low-rank skew-symmetric generator:
B =UV⊤−VU⊤, U,V ∈Rd×r,
(6)
where r ≪ d is the rank hyperparameter. The matrix B is skew-symmetric (B⊤ = −B) with
rank at most 2r, acting only on the 2r-dimensional subspace span(U,V ).
The rotation is recovered via the matrix exponential:
R=exp(B).
4.3 Efficient Subspace Computation
(7)
Computing exp(B) naively costs O(d3). We exploit the low-rank structure for O(dr2 + r3)
complexity.
Proposition 2 (Subspace Exponential). Let Q ∈ Rd×2r be orthonormal bases for span(U,V)
obtained via thin QR: Q = qr([U V]). Define the core skew-symmetric matrix
M=Q⊤BQ∈R2r×2r.
Then
R=exp(B) =Id+Q(exp(M)−I2r)Q⊤.
(8)
(9)
5
Algorithm 1 RoRA Forward Pass
Require: Input x ∈ Rd, learnable parameters U,V ∈ Rd×r
1: Q ←qr([U V])
▷ Thin QR; Q ∈Rd×2r
2: A ←Q⊤U, C ←Q⊤V
3: M ←AC⊤−CA⊤
4: Rcore ← exp(−M)
5: ∆R ←Rcore −I2r
6: xproj ← Q⊤x
7: xrot ← ∆Rxproj
8: return x+Qxrot
▷ Project to subspace
▷ M ∈R2r×2r skew-symmetric
▷ Compute R⊤ in core; O(r3)
▷ O(dr)
▷ O(r2)
▷ O(dr)
Proof. Let P = QQ⊤ be the orthogonal projector onto span(U,V). Since B maps into this
subspace and vanishes on its orthogonal complement, we have B = PBP = QMQ⊤. The
exponential power series exp(B) = ∞
k=0 
Bk
k! becomes
∞
exp(B) =
k=0
∞
(QMQ⊤)k
k!
=Q
k=0
Mk
k! Q⊤+(I−P)=I+Q(exp(M)−I2r)Q⊤. (10)
Algorithm 1 details the forward pass. Total complexity is O(dr2 + r3), matching LoRA’s O(dr)
for typical r.
4.4 Geometric Merging of RoRA Modules
A key advantage of RoRA is that merging happens in the Lie algebra so(d), where linear
combinations are meaningful and preserve orthogonality when mapped back to SO(d).
4.4.1 Merging in Lie Algebra Space
Consider two RoRA modules trained on tasks 1 and 2, parameterized by generators B1,B2 ∈
so(d):
Bi =UiV⊤
i −ViU⊤
i , Ri = exp(Bi) ∈ SO(d), i ∈ {1,2}.
(11)
Key Principle: Merging occurs in the generator space (Lie algebra), not in the rotation space
(Lie group).
For equal weighting, the merged generator is:
¯
B = 1
2B1 + 1
2B2.
Since so(d) is a vector space, ¯
(12)
B is skew-symmetric: ¯B⊤ = 1
2B⊤
1 + 1
2B⊤
2 = −1
2B1 − 1
2B2 = − ¯B.
The merged rotation is:
¯
R=exp(¯B) = exp 1
2B1 + 1
2B2 ∈ SO(d).
(13)
Proposition 3 (Lie Algebra Merging Preserves Orthogonality). Let B1,B2 ∈ so(d) and α1+α2 =
1 with αi ≥ 0. Then
¯
B =α1B1+α2B2 ∈so(d), ¯R =exp(¯B) ∈ SO(d).
Furthermore, ¯R lies on the geodesic connecting R1 and R2 on the manifold SO(d).
(14)
6
Proof. Skew-symmetry: ¯B⊤ = α1B⊤
1 +α2B⊤
2 = −α1B1 −α2B2 = −¯B, so ¯B ∈ so(d).
The matrix exponential maps so(d) → SO(d), so ¯R = exp( ¯B) ∈ SO(d).
For the geodesic property, the shortest path on SO(d) connecting R1 and R2 is parameterized by
R(t) = R1exp(t log(R⊤
1R2)), t ∈ [0,1].
(15)
At t =α2, when B1 and B2 commute or are close in norm, this approximates exp(α1B1 +α2B2)
to first order.
Remark 1. In contrast, naive averaging of rotations 1
2(R1+R2) does not preserve orthogonality
and can result in a non-orthogonal matrix with collapsed singular values (Proposition 1).
4.4.2 Low-Rank Merging via Subspace Alignment
When RoRA modules have different active subspaces span(U1,V1) ̸ = span(U2,V2), we must first
align them before merging in the Lie algebra.
Given two modules with representations (Q1,Rc1) and (Q2,Rc2), where Qi ∈ Rd× 2r are orthonor
mal subspace bases and Rci ∈ SO(2r) are core rotations:
Step 1: Subspace Alignment. Compute the overlap Q⊤
1Q2 ∈ R2r× 2r and its SVD:
Q⊤
1Q2 = USΣV⊤
S .
Construct the alignment rotation S ∈ SO(2r):
S =USdiag(1,...,1,det(USV ⊤
S ))V⊤
S ,
where the last diagonal entry is adjusted to ensure det(S) = 1.
Step 2: Transport Module 2 to Module 1’s Frame.
˜
Q2 =Q2S, ˜Rc2 =S⊤Rc2S.
This conjugation preserves the rotation action: ˜
Q2 
˜Rc2 
˜
Q⊤
2 = Q2Rc2Q⊤
2.
Step 3: Merge Subspaces. Average the aligned subspace bases and orthonormalize:
¯
Q=qr(Q1 + ˜Q2).
(16)
(17)
(18)
(19)
Step 4: Merge Core Rotations in Lie Algebra. Extract generators via matrix logarithm:
M1 =log(Rc1), M2 = log(˜Rc2) ∈ so(2r).
Merge in Lie algebra:
Map back to rotation:
¯
M=1
2M1+ 1
2M2 ∈so(2r).
¯
Rcore = exp( ¯
M)∈SO(2r).
Result: The merged RoRA module (¯Q, ¯Rcore) satisfies:
¯
R=Id+ ¯Q(¯Rcore −I2r) ¯Q⊤ ∈ SO(d).
(20)
(21)
(22)
(23)
This procedure ensures the merged adapter remains orthogonal, preserves energy, and lies on
the manifold geodesic connecting the two task-specific rotations.
7
Algorithm 2 RoRA Merge (Implementation)
Require: RoRA modules (Q1,Rc1) and (Q2,Rc2) with Qi ∈ Rd× 2r, Rci ∈ SO(2r)
1: USΣV⊤
S ← svd(Q⊤
1Q2)
2: S ←USdiag(1,...,1,det(USV ⊤
S ))V⊤
S
3: ˜Q2 ← Q2S, ˜Rc2 ← S⊤Rc2S
4: ¯Q ← qr(Q1 + ˜Q2)
5: M1 ←log(Rc1), M2 ←log(˜Rc2)
6: ¯M ← 1
2M1 + 1
2M2
7: ¯Rcore ← exp( ¯
M)
8: return Merged RoRA (¯Q, ¯Rcore)
4.5 Comparison with LoRA
LoRA [Hu et al., 2022] parameterizes updates as ∆W = α
rAB⊤ with A ∈ Rm× r,B ∈ Rd× r. The
adapted forward pass is
y =Wx+α
rA(B⊤x).
(24)
LoRA operates in the tangent space (additive updates), while RoRA operates on the manifold
(multiplicative orthogonal updates). Under small perturbations and unit normalization, LoRA
approximates first-order geodesic motion, but unlike RoRA, it can modify singular values.
Lemma 1 (LoRA as Tangent Approximation). Let u0,u1 be normalized weight directions with
geodesic (SLERP) u(t) on the unit sphere. Then u(t) = u0 + tξ + O(t2) where ξ ⊥ u0 is the
initial tangent. A rank-r LoRA update approximates ξ by its best rank-r approximation, yielding
f
irst-order agreement but no spectral preservation guarantee.
Decision Rule: Use RoRA when the base is strong and tasks are orthogonal-sufficient (Defini
tion 1). Use LoRA when tasks are spectral-necessary (Definition 2), requiring capacity expansion
in novel directions.
4.6 Computational Considerations
RoRA constrains adaptation to orthogonal transformations parameterized in the Lie algebra,
which yields mergeable geometry but introduces overhead relative to additive PEFT methods.
In particular, RoRA’s forward pass (Algorithm 1) performs (i) a thin QR factorization over the
active subspace and (ii) a matrix exponential on a 2r × 2r skew-symmetric core. This leads to
an overall per-layer complexity of
O(dr2 +r3),
(25)
where d is the feature dimension and r is the adapter rank. In contrast, LoRA-style additive
updates typically incur O(dr) additional work per layer.
This overhead reflects a design choice: RoRA explicitly tracks the orthogonal degrees of freedom
to enable manifold-aware interpolation and stable model merging. When the primary objective
is deployment-scale efficiency, recent work on isospectral transport provides a complementary
realization. In particular, GIFT [Sudjianto, 2026] enforces orthogonality via a Cayley parame
terization and computes the induced rotation in activation space using Woodbury-accelerated
solves, yielding near-LoRA training efficiency while retaining singular-value preservation. RoRA
and GIFT therefore separate concerns: RoRA emphasizes geometric diagnostics and merge
ability guarantees, whereas GIFT emphasizes computational throughput and transformer-scale
practicality.
8
In practice, RoRA’s overhead is modest when r is small (e.g., r ∈ {4,8,16}), and can be
further reduced through standard engineering choices such as caching or amortizing subspace
computations when U,V vary slowly across steps, or using truncated series/Pad´e approximations
for the 2r ×2r exponential when appropriate.
5 Experiments
We validate the spectral–orthogonal framework across four distinct adaptation regimes, chosen
to span the spectrum from orthogonal-sufficient to spectral-necessary scenarios.
5.1 Experimental Setup
Models and Implementation. For MNIST, we use a 2-layer MLP with hidden dimension
512. For CIFAR-10, we use ResNet-18 [He et al., 2016], pretrained on ImageNet where applicable.
RoRA modules are inserted after key layers (MLP hidden layers, ResNet residual blocks). We
compare:
• Pretrained/Base: Frozen base model
• Full Fine-tune: All parameters trainable
• LoRA: Additive low-rank updates, ranks {4,8,16}
• RoRA: Orthogonal updates, ranks {4,8,16}
Training. All methods use Adam optimizer [Kingma and Ba, 2015] with initial learning rate
selected per experiment. We train for 15 epochs on MNIST tasks, 30 epochs on CIFAR-10
adaptations, and 50 epochs on CIFAR-10 base training. For reproducibility, we use fixed random
seeds and report mean ± standard deviation across multiple seeds.
5.2 Experiment 1: MNIST Multi-Task Learning
Setup. We train a base MLP on standard 10-way MNIST digit classification (98.2% test
accuracy). We then add RoRA modules and two classification heads for simultaneous learning
of:
• Task A (Even/Odd): Binary label yA(d) = I[d is odd]
• Task B (Bit-Parity): Binary label yB(d) = popcount(d) mod 2
These tasks impose distinct geometric requirements on the same digit representations. We freeze
the base trunk and train only RoRA parameters and task heads using the joint loss L = LA+LB.
Results. Table 1 shows results averaged over 5 seeds.
RoRA achieves over 99% accuracy on both tasks simultaneously with rank as low as 4, demon
strating that reorienting the pretrained digit features suffices for these distinct binary labelings.
The flat performance across ranks indicates this is an orthogonal-sufficient regime.
9
Table 1: MNIST Multi-Task Learning: Even/Odd and Bit-Parity classification with shared
frozen trunk. RoRA achieves near-perfect accuracy across all ranks with minimal variance.
Method
Task A Accuracy Task B Accuracy
Base (frozen)
98.17%
98.17%
RoRA Rank 4
RoRA Rank 8
RoRA Rank 16
99.19 ± 0.06
99.22 ± 0.02
99.21 ± 0.05
99.19 ± 0.06
99.22 ± 0.02
99.21 ± 0.05
Table 2: MNIST Add-9: Adapting from digits 0–8 to full 0–9. When the base model lacks
features for novel classes, LoRA shows marginal gains.
Method
Rank 4
Rank 8
Rank 16
Base (0–8 only)
98.53 ± 0.05
RoRA
LoRA
98.25 ± 0.08 98.24±0.08 98.25±0.06
98.32 ± 0.06 98.34±0.05 98.34±0.03
5.3 Experiment 2: MNIST Novel Class Adaptation (Add-9)
Setup. To test spectral-necessary scenarios, we train a base model on digits 0–8 only (9-way
classification, 98.5% accuracy on the restricted set). We then adapt to recognize all digits 0–9,
where digit 9 is entirely novel. This requires the model to learn new discriminative features not
present in the base.
Results. Table 2 shows results over 5 seeds.
LoRA outperforms RoRA by ≈ 0.09%, a small but consistent advantage. This validates our
hypothesis: when the base fundamentally lacks necessary features (digit 9 never seen), additive
updates that can expand capacity in new directions are beneficial. However, the advantage is
modest, suggesting the pretrained feature extractor generalizes reasonably even to unseen digits.
5.4 Experiment 3: CIFAR-10 Small-Data Fine-Tuning
Setup. We start with ResNet-18 pretrained on ImageNet (strong base with generic features)
and fine-tune on CIFAR-10 with severely limited data: N ∈ {50,100,250} stratified samples per
class. This tests whether RoRA’s spectral preservation reduces overfitting when data is scarce.
We compare 3 seeds each.
Results. Table 3 shows results.
RoRA outperforms LoRA by 2–10% across all data regimes, with the gap widening at N = 250.
This supports our hypothesis: under data scarcity with a strong base, preventing spectral drift
reduces overfitting. Full fine-tuning dominates both methods, indicating that when sufficient
data and compute are available, unconstrained capacity adjustment is optimal. However, RoRA
offers a favorable trade-off for parameter efficiency.
10
Table 3: CIFAR-10 small-data fine-tuning from ImageNet-pretrained ResNet-18. RoRA consis
tently outperforms LoRA under extreme data scarcity, with full fine-tuning dominating when
sufficient capacity change is permitted.
Method
N =50
N =100
N =250
Pretrained (frozen) 9.90 ± 2.17 8.73±0.36 12.40±2.16
LoRA Rank 8
10.49 ± 0.36 9.90±0.82 11.55±1.36
RoRA Rank 8
12.34 ± 0.36 11.81±0.91 21.53±1.33
Full Fine-tune
20.88 ± 0.08 25.61±0.94 39.45±0.95
Table 4: CIFAR-10 grayscale→RGB adaptation. RoRA dramatically outperforms LoRA,
indicating color information can be recovered via geometric feature rotation.
Method
Rank 4
Rank 8
Rank 16
RoRA 68.13±0.03 71.66±0.49 73.78±0.48
LoRA 45.35±0.89 55.54±0.62 57.59±0.37
5.5 Experiment 4: CIFAR-10 Grayscale-to-RGB Adaptation
Setup. We train ResNet-18 from scratch on grayscale CIFAR-10 images (1 channel, 50 epochs,
achieving ∼70% accuracy). We then adapt to RGB CIFAR-10 (3 channels) by inserting adapters
after the first convolutional layer and key residual blocks. This tests whether color information can
be recovered by rotating existing grayscale features (RoRA) or requires new spectral directions
(LoRA).
Results. Figure 1 and Table 4 show results over 3 seeds.
RoRA achieves 73.8% accuracy at rank 16, outperforming LoRA by over 16%. This is our most
striking result: color information can be recovered by rotating grayscale features rather than
adding new directions. The base model’s grayscale representations already contain sufficient
structure; they simply need geometric reorientation to handle chromatic inputs. This challenges
the intuition that missing modalities require spectral expansion, validating RoRA’s geometric
approach.
5.6 Discussion
Our experiments reveal clear patterns:
• Orthogonal-sufficient regimes (multi-task, small data, missing modality): RoRA
matches or exceeds LoRA, often by substantial margins. Spectral preservation prevents
overfitting and enables effective adaptation through basis rotation alone.
• Spectral-necessary regimes (novel classes): LoRA shows small but consistent gains
when the base fundamentally lacks representational capacity for new concepts.
• Rank trends: RoRA shows stable or improving performance with rank, while LoRA can
overfit at high rank when spectral change is unnecessary.
These findings support our spectral–orthogonal framework and provide empirical guidance for
method selection in practice.
11
Figure 1: CIFAR-10 grayscale→RGB: RoRA outperforms LoRA by 16–23% across all ranks.
Error bars show standard deviation over 3 seeds.
6 Limitations and Future Work
While RoRA demonstrates strong performance across multiple regimes and provides a princi
pled spectral–orthogonal view of adaptation and merging, several limitations warrant further
investigation.
Computational overhead relative to additive PEFT. RoRA’s forward pass requires (i)
subspace orthonormalization and (ii) a small matrix exponential on the active 2r-dimensional
subspace, resulting in O(dr2 + r3) work per adapted layer. Although this is tractable for
small r, it introduces non-negligible constant factors compared to additive low-rank updates.
A promising direction is to develop faster approximations that preserve the key geometric
properties (orthogonality and stable merging), including improved caching strategies, more
efficient exponential approximations, and fused-kernel implementations on GPU.
Rank selection and adaptive geometry. Like LoRA, RoRA introduces a rank hyperparam
eter r, but the effective active subspace dimension is 2r due to the skew-symmetric generator
structure. Selecting r remains task-dependent. Future work should investigate adaptive rank allo
cation (e.g., per-layer budgets), diagnostics-driven rank tuning using spectral–orthogonal criteria,
and hybrid adapters that interpolate between orthogonal-only and mixed spectral–orthogonal
updates.
Theory: optimization and generalization in orthogonal-only adaptation. RoRA is
motivated by the hypothesis that many strong-base adaptation problems are orthogonal-sufficient.
While we provide geometric intuition and empirical evidence, a full theoretical characterization
remains open. Important directions include: (i) sufficient conditions under which orthogonal
only adaptation is optimal (or near-optimal), (ii) generalization bounds that quantify the
regularization effect of isospectral constraints under data scarcity, and (iii) optimization analyses
on SO(d) for low-rank generators.
Merging beyond two tasks and non-commutativity. RoRA merges adapters by interpo
lating in the Lie algebra and mapping back to SO(d). However, when combining many tasks, the
12
non-commutativity of rotations can introduce higher-order effects, and different task weightings
may yield different merged optima. Future work should develop principled multi-task merging
rules, including weighting schemes based on task similarity, curvature-aware interpolation on
SO(d), and connections to barycenters on matrix manifolds.
Evaluation breadth and scaling studies. Our experiments emphasize regimes designed
to separate orthogonal-sufficient from spectral-necessary adaptations. A broader evaluation is
needed, including larger backbones, more diverse datasets, and stress tests under distribution
shift and continual learning. Recent evidence on isospectral transport at transformer scale
[Sudjianto, 2026] suggests that spectrum-preserving adaptation is compatible with modern
architectures; extending RoRA’s merging-centric formulation to such large-scale settings and
benchmarking end-to-end efficiency and quality is a key next step.
7 Conclusion
We have introduced Rotational Rank Adaptation (RoRA), a parameter-efficient fine-tuning
method grounded in the geometric principle that many adaptation tasks require only basis
reorientation, not capacity expansion. By constraining updates to orthogonal transformations
via low-rank Lie algebra generators, RoRA preserves feature energy, avoids spectral drift, and
enables stable geometric merging.
Our comprehensive experiments across four distinct regimes—multi-task learning, novel class
adaptation, small-data fine-tuning, and missing modality transfer—validate the spectral–orthogonal
framework. RoRA consistently outperforms additive methods when the base model is strong
and tasks are orthogonal-sufficient, with particularly dramatic advantages (up to 16%) in the
grayscale-to-RGB regime. Conversely, we identify scenarios (novel classes) where spectral
adaptation remains beneficial, providing clear decision criteria.
These findings suggest a paradigm shift in thinking about adaptation: rather than universally
applying additive updates, we should first assess whether the base model’s feature space
is adequate. If so, geometric reorientation via RoRA offers superior parameter efficiency,
generalization, and mergability. As foundation models grow in capability, we expect orthogonal
sufficient regimes to become increasingly common, making RoRA’s geometric approach central
to scalable, compositional AI systems.
References
P.-A. Absil, R. Mahony, and R. Sepulchre. Optimization Algorithms on Matrix Manifolds.
Princeton University Press, 2009.
S. K. Ainsworth, J. Hayase, and S. Srinivasa. Git re-basin: Merging models modulo permutation
symmetries. In International Conference on Learning Representations (ICLR), 2023.
T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer. QLoRA: Efficient finetuning of
quantized LLMs. In Advances in Neural Information Processing Systems (NeurIPS), 2023.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2016.
N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. de Laroussilhe, A. Gesmundo, M. At
tariyan, and S. Gelly. Parameter-efficient transfer learning for NLP. In International Conference
on Machine Learning (ICML), pages 2790–2799, 2019.
13
E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. LoRA:
Low-rank adaptation of large language models. In International Conference on Learning
Representations (ICLR), 2022.
G. Ilharco, M. T. Ribeiro, M. Wortsman, L. Schmidt, H. Hajishirzi, and A. Farhadi. Editing
models with task arithmetic. In International Conference on Learning Representations (ICLR),
2023.
D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations (ICLR), 2015.
X. L. Li and P. Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Annual
Meeting of the Association for Computational Linguistics (ACL), pages 4582–4597, 2021.
Y. Li, N. Tian, K. Kreis, F. Zhou, and J. Lucas. Preventing gradient attenuation in Lipschitz
constrained convolutional networks. In Advances in Neural Information Processing Systems
(NeurIPS), 2019.
C. H. Martin and M. W. Mahoney. Implicit self-regularization in deep neural networks: Evidence
from random matrix theory and implications for learning. Journal of Machine Learning
Research, 22(165):1–73, 2021.
M. S. Matena and C. A. Raffel. Merging models with Fisher-weighted averaging. In Advances in
Neural Information Processing Systems (NeurIPS), 2022.
T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. Spectral normalization for generative
adversarial networks. In International Conference on Learning Representations (ICLR), 2018.
A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of learn
ing in deep linear neural networks. In International Conference on Learning Representations
(ICLR), 2014.
E. Vorontsov, C. Trabelsi, S. Kadoury, and C. Pal. On orthogonality and learning recurrent
networks with long term dependencies. In International Conference on Machine Learning
(ICML), pages 3570–3578, 2017.
M. Wortsman, G. Ilharco, S. Y. Gadre, R. Roelofs, R. Gontijo-Lopes, A. S. Morcos, H. Namkoong,
A. Farhadi, Y. Carmon, S. Kornblith, and L. Schmidt. Model soups: Averaging weights of mul
tiple fine-tuned models improves accuracy without increasing inference time. In International
Conference on Machine Learning (ICML), pages 23965–23998, 2022.
Y. Yoshida and T. Miyato. Spectral norm regularization for improving the generalizability of
deep learning. arXiv preprint arXiv:1705.10941, 2017.
Q. Zhang, M. Chen, A. Bukharin, P. He, Y. Cheng, W. Chen, and T. Zhao. AdaLoRA: Adaptive
budget allocation for parameter-efficient fine-tuning. In International Conference on Learning
Representations (ICLR), 2023.
A. Sudjianto. GIFT: Geometric Isospectral Fine-Tuning. SSRN, 2026. Available at SSRN:
5995254. DOI: 10.2139/ssrn.5995254.
